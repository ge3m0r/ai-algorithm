## 第二周
##### 监督学习
监督学习提供答案，然后按照答案数据进行模型训练，孩子在识别动物的时候，我们会告诉它那个东西是猫，那个动物是兔子，久而久之，孩子就会知道猫的特征，但是这个时候孩子无法知道我们告诉他之外的内容。
如果下次当其遇到老虎，它在不知道这个物种可能会误认为是猫，只有我们告诉它是老虎才会进一步知道。

* 应用场景
推荐系统的点击率的预估，一般包含用户团队，负责用户增长，一个包含商业团队，让用户进行商业付款的团队。

##### 损失函数
监督学习中最关键的损失函数（loss function）
监督学习分为回归和分类两类任务（Regression and Classification)
回归任务：
损失函数类型选择
* 差值求和
因为预测值和实际由正负区别，加和到一起可能会相互抵消，因此线程求差会由很大问题
* 绝对值误差
数学上不完美，不可微，无法求导
优点是对于离群点不敏感
* 均方误差
可微，对于离群点会放大

分类任务：
损失函数的选择：
* 0-1 loss
函数不可微，函数是分段函数，分类任务预测会预测不同结果的概率
* KL 散度
模型预测使的一个不同类型结果的概率，本质上需要计算真实结果的概率分布和预测结果分布的概率差别
KL 散度是相对熵，形容两个散度的差异
* 交叉熵
KL 散度 = A(真实数据自身混乱程度) + B（模型预测与真实数据差异）
其中 A 是信息熵，是数据本身的熵， 交叉熵就是 B ，表示其中熵的差别

##### 线性模型
现代 AI 的两阶段工作范式
1. 表示学习
利用预训练模型，将原始和非结构化数据转化为高质量的特征向量
做的是一个编码器或者特征向量提取器的角色。
2. 任务决策
将信息量巨大的特征向量输入到更简单，专门针对下游任务的模型，完成最终的分类和回归
其中大模型负责深度理解，小模型负责快速决策

线性模型希望表示每个维度对于模型分类之间加权贡献，通过训练得到每个维度的贡献值。
概率映射：
将实际数据输出到 0-1 的概率熵
* sigmoid 函数
加权求和 + sigmod 函数结果就是逻辑回归函数
* softmax 函数

在线性模型转化为具体内容：
我们将预测内容数字转化为结果，在训练过程，如果对文本比方说北京进行简单的数值编码
北京：1 上海：2 
但是在同样条件下，上海是北京的两倍，相当于在同一个维度的编码不平等
* 独热编码
对每个文本类别按照都分配一个维度，每个城市是一个单独向量表示
独热编码的缺陷：文本之间还是由相似的维度，北京和上海并不是完全互斥的，在一些维度相同的。

##### 解决方法
1. 输入和嵌入
将文本转化为特征向量
2. 拼接和堆叠
3. 学习和更新权重
4. 输出

##### 决策树和集成学习
决策树通过构建一系列特征然后判断最终结果
数据集的纯度：数据分类的熵
信息增益：
表示信息熵的大小的值，会选择信息增益最大的特征

###### 缺陷
1. 过拟合
样本由偶然性
2. 不稳定
训练数据微小变动，可能会引起结构较大变化

##### 集成学习
将几个弱模型集成起来，形成大的模型

##### 无监督学习
纯粹无监督学习目前应用较少
作用：
聚类
降维
关联规则

##### KNN 到向量检索
KNN 是一种无监督学习方法，一般用于向量的检索，计算topk 内容，rag 里边knn内容
查询和检索：
构建类似 kNN 的性能更强的算法

K-Means 为数据寻找天然的群体中心
PCA 降维 将高维度降低到低维度

##### 模型评估
损失函数是训练过程中的监督
模型评估指标是训练完成后评估结果
回归问题：
1. 平均绝对误差
2. 均方根误差
3. 决定系数
错误，模型预测错误带来的损失：
误报，漏报，一般是宁可漏报也不要误报
欺诈：宁可误报也不要漏报

阈值怎么选择也是一个问题。
AUC 好的样本高于负样本概率高
模型预测需要把目标类别排在非目标类别之前。
